\chapter{Introduction}

Anyone reading this book should be familiar with the notion of the first, second and higher derivatives, \textit{e.g.}, for $f(t) = t^3 + 5 t^2 + 2$, 
\begin{align}
  \frac{df}{dt}(t) &= 3 t^2 + 10 t \\
  \frac{d^2f}{dt^2}(t) &= 6t + 10, 
  \label{eq:derivs}
\end{align}
\textit{etc.} Also we naturally think of integrals in an antiderivative sense, \textit{e.g.},
\begin{align}
  \int f(t) dt &= \frac{1}{4} t^4 + \frac{5}{3} t^3 + 5 t^2 + c 
  \label{eq:integrals}
\end{align}
and we adopt a notation of $f^{(1)}(t)$ as the first derivative, $f^{(2)}(t)$ as the second derivative and $f^{(n)}(t)$ as the $n$th derivative as well as $f^{(-1)}(t)$ as $f$ integrated one time, $f^{(-2)}$ as $f$ integrated two times, \textit{etc.}

A mathematically curious reader may already be wondering if there are any derivatives ``in between'' the integer ones.
For example, is there a one-half derivative:
\begin{equation}
  \frac{d^\frac{1}{2} f}{d t^\frac{1}{2}}(t) = f^{\left(\frac{1}{2}\right)} = ?.
  \label{eq:halfderiv}
\end{equation}
There is not an immediate obvious answer to this because of the fact that the integer order derivative (as is the integral) is defined as a limit
\begin{equation}
  \frac{df}{dt}(t) = \lim_{\Delta t \rightarrow 0} = \frac{f\left(t + \Delta t \right) - f\left(t\right)}{\Delta t}
  \label{eq:limdef}
\end{equation}
and that is a discrete operation. There is not a natural half way to do it.

Basically we want to generalize the notion of the derivative. In a sense, if we define something to give the, say $\alpha$ derivative where $\alpha \in \mathbb R$, \textit{i.e.}, $\alpha$ is a real number, then all we really need is that when $\alpha$ is an integer we get the usual definition of that integer order derivative. In between there may be lots of different options (there are!), but it makes sense to set some other basic requirements we want a fractional-order derivative to satisfy.

\begin{example}
  Consider $f(t) = t^2$ with the first and second derivatives $f^{(1)}(t) = 2 t$ and $f^{(2)}(t) = 2$, respectively. We should expect that the $1/2$ derivative is, in some qualitative sense, ``between'' $f(t)$ and $f^{(1)}(t)$, and that the $3/2$ derivative is ``between'' the first and second derivatives, as is illustrated in Figures~\ref{fig:fracidea1} and \ref{fig:fracidea2}.

  \begin{figure}
    \centering
    \subimport{figs/}{fracidea}
    \caption{The zeroth and first derivatives of $f(t)=t^2$, (blue and red curves). A prospective half derivative (gold).}
    \label{fig:fracidea1}
  \end{figure}

  \begin{figure}
    \centering
    \subimport{figs/}{fracidea2}
    \caption{The first and second derivatives of $f(t)=t^2$ (blue and red curves) with a prospective $3/2$ derivative (gold).}
    \label{fig:fracidea2}
  \end{figure}

  We also expect that as we vary the order of the derivative, say from $0$ to $1$, for low values of the order the result is near the zeroth derivative, and for values of the fractional order near one, it is near the first derivative. This is illustrated in Figure~\ref{fig:fracidea3}.

  \begin{figure}
    \centering
    \subimport{figs/}{fracidea3}
    \caption{The function (blue) and the first derivative (red). The $0.2$, $0.4$, $0.6$ and $0.8$th (yellow, purple, green, light blue) order derivatives ``move'' from the zeroth derivative to the first derivative.}
    \label{fig:fracidea3}
  \end{figure}
\end{example}

This example motivates our first desirable attribute of a fractional derivative.

\begin{attribute}
  For a fractional derivative, $f^{\left( \alpha \right)}(t)$, if $\alpha$ is near an integer value, we expect the $\alpha$ derivative to be near that integer derivative of $f(t)$. As $\alpha$ varies between integer values, we expect that $f^{\left( \alpha \right)}(t)$ varies in a reasonable manner between those integer values.
\end{attribute}

\section{Some Basic Cases}
\subsection{Sine and Cosine Functions}
As a first step into some real functions, we consider sine and cosine.

\begin{example}
  Consider $f(t) = \sin(t)$. The nice thing about sines and cosines are their relatively simple derivatives. In fact, from the pattern
  \begin{align*}
    \frac{d}{dt} \sin(t) &= \cos(t) \\
    \frac{d^2}{dt^2} \sin(t) &= -\sin(t)
  \end{align*}
  as illustrated in Figure~\ref{fig:fracidea4}, it is clear that the derivative for this function just shifts it to the left by $\pi/2$. So clearly we would expect that the $1/2$ derivative just shifts it to the left by $\pi/4$, or the $1/2$ integral shifts it right by $\pi/3$, \textit{etc}. Therefore, a reasonable guess for the half derivative would be as illustrated in Figure~\ref{fig:fracidea5}.

  \begin{figure}
    \centering
    \subimport{figs/}{fracidea4}
    \caption{First (red) and second (gold) derivative of $\sin(t)$ (blue). The arrows indicate that derivative is just shifts the curve to the left by $\pi/2$ for each derivative.}
    \label{fig:fracidea4}
  \end{figure}

  \begin{figure}
    \centering
    \subimport{figs/}{fracidea5}
    \caption{Reasonable assumption for the half derivative (red) of $\sin9t)$ (blue).}
    \label{fig:fracidea5}
  \end{figure}

\end{example}

\subsection{Monomials and Polynomials}

Being able to take the fractional derivative of sine and cosine functions is nice, but we would like to do more. The next easiest class of functions is polynomials. Consider 
\begin{equation}
  f(t) = t^k
  \label{eq:poly}
\end{equation}
which is easy to differentiate a few times and figure out the pattern:
\begin{align}
  \frac{df}{dt}(t) &= k t^{k-1} \\
  \frac{d^2f}{dt^2}(t) &= k \left( k-1 \right) t^{k-2} \\
  \frac{d^3f}{dt^3}(t) &= k \left( k-1 \right) \left( k - 2 \right) t^{k-3} \\
  \vdots &= \vdots \\
  \frac{d^nf}{dt^n}(t) &= \frac{k!}{\left( k - n \right)!} t^{k-n}, \qquad n \leq k.
  \label{eq:polyderivs}
\end{align}
Since we are looking to define a fractional derivative of $t^k$, we need to see what is allowed and not allowed for $n$ to take on fractional values in Equation~\ref{eq:polyderivs}. The exponent of $t$ can be a fraction (more about that later, but engineers are so used to it they may not remember where it came from). What is definitely a problem, though, is the factorial in the denominator: if $k$ is a natural number and $n$ is not an integer, then the factorial is not defined.

The factorial function is just a series of values, so it seems we can generalize the derivative of $t^k$ if we can find a curve through the factorial values. In fact, of course, it has been done and it is the gamma function defined by
\begin{equation}
  \Gamma\left(z\right) = \int_0^\infty x^{z - 1} e^{-x} dx
  \label{eq:gamma}
\end{equation}
which is plotted for positive real values of $z$ in Figure~\ref{fig:gamma}. Clearly, for integer values
\begin{equation}
  z! = \Gamma\left( z + 1 \right).
\end{equation}

\begin{figure}
  \centering
  \subimport{figs/}{gamma}
  \caption{Plot of $\Gamma(z)$ (blue curve) and some factorials, $\left(z - 1\right)!$ (red).}
  \label{fig:gamma}
\end{figure}

So, returning to Equation~\ref{eq:polyderivs}, it seems that all we need to do to define a fractional derivative is to replace the factorial in the denominator with the gamma function shifted by one. Purely for aesthetics, we might as well use a gamma function in the numerator as well, which also then would allow for a fractional $k$. So, we have the following seemingly legitimate definition of a fractional derivative for a monomial.

\begin{definition}
  For the monomial, $t^k$, $k \in \mathbb R$, define the fractional derivative 
  \begin{equation}
    \frac{d^\alpha}{dt^\alpha} t^k = \frac{\Gamma\left( k + 1 \right)}{\Gamma \left( k + 1 - \alpha \right)} t^{k - \alpha}
    \label{eq:monomialfrac}
  \end{equation}
  for real values of $\alpha$, \textit{i.e.}, $\alpha \in \mathbb R$.
  \label{def:monomialfracderiv}
\end{definition}

This definition was used to make the fractional derivative curves in Figures~\ref{fig:fracidea1} through
\ref{fig:fracidea3}, so in a sense it has been validated. Also, because everything we have done is linear in $t$, we can
use this definition for monomials and extend it term-by-term to polynomials. 

For fun, we will do and example with a different $k$ and also include negative values for $\alpha$ to see if integral-like ideas appear.

\begin{example}
  Consider $f(t) = t^\frac{1}{2} = \sqrt{t}$. Various fractional order derivatives and integrals are illustrated in Figure~\ref{fig:monex}.

  \begin{figure}
    \centering
    \subimport{figs/}{monex}
    \caption{Various fractional derivatives using Definition~\ref{def:monomialfracderiv} for $f(t) = \sqrt{t}$. The thick blue curve is the function, or zeroth derivative. The red curve is the $\alpha = -1$ derivative, which does correspond to the integral. The gold curve is the $\alpha = -1/2$ derivative, or the $1/2$ integral. The green and light blue curves are the $1/2$ and first derivatives, respectively.}
    \label{fig:monex}
  \end{figure}
\end{example}

At this point we can compute fractional derivatives and integrals of
\begin{itemize}
  \item sines and cosines
  \item monomials
  \item by extension from monomials, polynomials if we do them term-by-term.
\end{itemize}

Before we generalize further, in order to develop a very important property of the fractional derivatives, we need to go a long way back and consider fractional exponents.

\section{The Law of Indices}

Engineers deal with fractional and negative exponents so often that it is easy to lose track of why they actually make sense. The exponent is defined for natural numbers (integers greater than zero) as the number of times the base is multiplied by itself, \textit{i.e.},
\begin{equation}
  t^n = \underbrace{t \times t \times t \cdots t \times t}_{\mbox{$n$ times}}.
  \label{eq:power}
\end{equation}
An obvious property of this is that for two natural number exponents
\begin{align}
  \left(t^n\right) \times \left( t^m \right) &=  \left( \underbrace{t \times t \times t \cdots t \times t}_{\mbox{$n$ times}}
\right) \times \left( \underbrace{t \times t \times t \cdots t \times t}_{\mbox{$m$ times}}
\right) \\
&= 
 \underbrace{t \times t \times t \cdots t \times t}_{\mbox{$n + m$ times}} \\
 &= t^{n + m}
 \label{eq:indices}
\end{align}
which also immediately leads to
\begin{equation}
  \left( t^n \right)^p = t^{n \times p}.
  \label{eq:indices2}
\end{equation}
This notion of adding indices can be used to \emph{define} negative and fractional exponents by requiring that Equations~\ref{eq:indices} and \ref{eq:indices2} hold for all rational values as well (negative values, zero and fractional values). 

For negatives values, consider $n$ and $m$ to be positive integers with $n > m$, and if we require that $\left( t^n \right) \times \left( t^{-m} \right) = t^{n-m}$, then the only way for the $-m$ to take away powers is for it to mean division, or in too much detail
\begin{equation}
  t^n \times t^{-m} = \frac{\overbrace{t \times t \times t \cdots t \times t}^{\mbox{$n$ times}}}{\underbrace{t \times t \cdots t \times t}_{\mbox{$m$ times}}} = \underbrace{t \times t  \cdots t \times t}_{\mbox{$n-m$ times}}.
  \label{eq:indices3}
\end{equation}

For fractional values Equation we can use Equation~\ref{eq:indices2} so that
\begin{equation}
  t^\frac{n}{m} = y \qquad \Longrightarrow \qquad \left( t^\frac{n}{m} \right)^m = y^m \qquad \Longrightarrow \qquad t^n = y^m
\end{equation}
which gives the meaning that $y$ is the number that if you raise it to the $m$th power gives $t$ to the $n$th power, e.g. in the simple case of $1/2$, $y$ is the number that if you square you get $t$.

These exercises in indices are important, because they hold for integer order derivatives
\begin{equation}
  \frac{d^n}{dt^n} \left( \frac{d^m}{dt^m} f(t) \right) = \frac{d^{n+m}f}{dt^{n+m}}(t).
\end{equation}
By insisting that the same property hold when $n$ and $m$ are fractional, will help in generalizing the derivative to non-integer values for a larger class of functions that simple sines, cosines and polynomials. In fact, we may as well call it an attribute.

\begin{attribute}
  For real values of $\alpha$ and $\beta$
  \begin{equation}
    \frac{d^\alpha}{dt^\alpha} \left( \frac{d^\beta}{dt^\beta} f(t) \right) = \frac{d^{\alpha + \beta}f}{dt^{\alpha + \beta}}(t).
    \label{eq:derivindices}
  \end{equation}
\end{attribute}

Even in integer order calculus, integration and differentiation are not exactly inverses because an indefinite integral will have a constant of integration. In other words, if we take $f(t)$ and differentiate it and then integrate it, we get $f(t) + c$, but if we integrate and then differentiate, we get $f(t)$. The idea is clear enough, but it turns out that this complication does affect things.


\section{The Frequency Domain}

Multiplication by $s$ in the frequency domain corresponds to differentiation by $t$ in the time domain if we use the
usual variables in the Laplace transform
\begin{equation}
\mathcal{L} \left\{ f(t) \right\} = \int_{0^-}^\infty f(t) e^{-s t} dt = F(s),
\end{equation}
\textit{i.e.}, 
\begin{equation}
\mathcal L \left\{ \frac{df}{dt}(t) \right\} = s F(s) - f(0)
\end{equation}
or assuming zero initial conditions
\begin{equation}
\mathcal L \left\{ \frac{df}{dt}(t) \right\} = s F(s).
\end{equation}
Higher derivatives are just increased exponents on the $s$, \textit{e.g.}, 
\begin{equation}
\mathcal L \left\{ \frac{d^n f}{dt^n}(t) \right\} = s^n F(s)
\end{equation}
again assuming zero initial conditions.

Of course, the half derivative then would correspond to $s$ raised to the one-half power:
\begin{equation}
  \mathcal L \left\{ \frac{d^\frac{1}{2} f}{d t^\frac{1}{2}}(t) \right\} = s^\frac{1}{2} F(s).
\end{equation}
